# codex-api

Expose `codex exec` as a small OpenAI-compatible HTTP API (local or deployable).

## Requirements

- Python 3.10+ (tested on 3.13)
- `codex` CLI installed and authenticated on the machine that runs this server

## Install

### Option A: uv (recommended)

```bash
uv sync
```

### Option B: pip

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

## Run

By default it only binds to localhost (`127.0.0.1`) and uses `--sandbox read-only`.

### With `.env` + helper script (recommended)

```bash
cp .env.example .env
./scripts/serve.sh
```

```bash
export CODEX_WORKSPACE=/path/to/your/workspace
export CODEX_GATEWAY_TOKEN=devtoken   # optional but recommended
uv run uvicorn main:app --host 127.0.0.1 --port 8000
```

If you installed via pip + activated a venv:

```bash
uvicorn main:app --host 127.0.0.1 --port 8000
```

To allow “online access”, bind to `0.0.0.0` and put it behind a reverse proxy / firewall:

```bash
uv run uvicorn main:app --host 0.0.0.0 --port 8000
```

## API

- `GET /healthz`
- `GET /v1/models`
- `POST /v1/chat/completions` (supports `stream`)

### Example (non-stream)

```bash
curl -s http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer devtoken" \
  -d '{
    "model":"gpt-5.2",
    "messages":[{"role":"user","content":"总结一下这个仓库结构"}],
    "reasoning": {"effort":"low"},
    "stream": false
  }'
```

### Example (stream)

```bash
curl -N http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer devtoken" \
  -d '{
    "model":"gpt-5-codex",
    "messages":[{"role":"user","content":"用一句话解释这个项目的目的"}],
    "stream": true
  }'
```

## Configuration (env vars)

- `CODEX_WORKSPACE`: directory passed to `codex exec --cd`
- `CODEX_CLI_HOME`: override HOME for the `codex` subprocess (default: `./.codex-gateway-home`)
- `CODEX_USE_SYSTEM_CODEX_HOME`: `1/0` (default: `0`) use your normal `~/.codex` config instead of the gateway home
- `CODEX_MODEL`: default model id (default: `gpt-5-codex`)
- `CODEX_MODEL_ALIASES`: JSON map of request model -> real model (e.g. `{"autoglm-phone":"gpt-5.2"}`)
- `CODEX_ADVERTISED_MODELS`: comma-separated list for `GET /v1/models` (defaults to `CODEX_MODEL`)
- `CODEX_MODEL_REASONING_EFFORT`: `low|medium|high|xhigh` (default: `high`)
- `CODEX_SANDBOX`: `read-only` | `workspace-write` | `danger-full-access` (default: `read-only`)
- `CODEX_APPROVAL_POLICY`: `untrusted|on-failure|on-request|never` (default: `never`)
- `CODEX_DISABLE_SHELL_TOOL`: `1/0` (default: `1`) disable Codex shell tool so responses stay "model-like" and avoid surprise command executions
- `CODEX_ENABLE_SEARCH`: `1/0` (default: `0`)
- `CODEX_ADD_DIRS`: comma-separated extra writable dirs (default: empty)
- `CODEX_SKIP_GIT_REPO_CHECK`: `1/0` (default: `1`)
- `CODEX_GATEWAY_TOKEN`: if set, require `Authorization: Bearer ...`
- `CODEX_TIMEOUT_SECONDS`: (default: `600`)
- `CODEX_MAX_CONCURRENCY`: (default: `2`)
- `CODEX_MAX_PROMPT_CHARS`: (default: `200000`)
- `CODEX_SUBPROCESS_STREAM_LIMIT`: asyncio stream limit for subprocess pipes (default: `8388608`)
- `CODEX_CORS_ORIGINS`: comma-separated origins for CORS (default: empty/disabled)
- `CODEX_SSE_KEEPALIVE_SECONDS`: send SSE keep-alives to prevent client read timeouts (default: `5`)
- `CODEX_STRIP_ANSWER_TAGS`: `1/0` (default: `0`) strip `</answer>` for action-parsing clients (e.g. Open-AutoGLM)
- `CODEX_ENABLE_IMAGE_INPUT`: `1/0` (default: `1`) decode OpenAI-style `image_url` parts and pass them to `codex exec --image`
- `CODEX_MAX_IMAGE_COUNT`: (default: `4`)
- `CODEX_MAX_IMAGE_BYTES`: (default: `8388608`)
- `CODEX_DEBUG_LOG`: `1/0` (default: `0`) log prompt/events/response to server logs
- `CODEX_LOG_MAX_CHARS`: truncate long log lines (default: `4000`)

## Multi-provider (optional)

If you have other agent CLIs installed, you can select them by prefixing `model`:

- Codex CLI: `"gpt-5.2"` (default) or any Codex model id
- Cursor Agent: `"cursor-agent:<model>"` or `"cursor:<model>"` (e.g. `cursor:sonnet-4-thinking`)
- Claude Code: `"claude:<model>"` or `"claude-code:<model>"` (e.g. `claude:sonnet`)
- Gemini CLI: `"gemini:<model>"` or `"gemini"` (e.g. `gemini:gemini-2.0-flash`)

Optional env vars:

- `CURSOR_AGENT_BIN`, `CLAUDE_BIN`, `GEMINI_BIN`: override the CLI binary names/paths
- `CURSOR_AGENT_API_KEY` / `CURSOR_API_KEY`: Cursor authentication for `cursor-agent`
- `CURSOR_AGENT_MODEL`, `CLAUDE_MODEL`, `GEMINI_MODEL`: default model when the prefix doesn’t include `:<model>`

## Security notes

You are exposing an agent that can read files and run commands depending on `CODEX_SANDBOX`.
Keep it private by default, use a token, and run in an isolated environment when deploying.

## Performance notes (important)

If your normal `~/.codex/config.toml` has many `mcp_servers.*` entries, **Codex will start them for every `codex exec` call**
and include their tool schemas in the prompt. This can add **seconds of startup time** and **10k+ prompt tokens** per request.

For an HTTP gateway, it’s usually best to run Codex with a minimal config (no MCP servers).

This project **defaults** to a gateway-local HOME at `./.codex-gateway-home` so it doesn’t inherit your global `~/.codex/config.toml`.
On first run it will try to copy `~/.codex/auth.json` into `./.codex-gateway-home/.codex/auth.json` (so you don’t have to).

If you want to set it up manually or customize it:

```bash
mkdir -p .codex-gateway-home/.codex
cp ~/.codex/auth.json .codex-gateway-home/.codex/auth.json   # or set CODEX_API_KEY instead
cat > .codex-gateway-home/.codex/config.toml <<'EOF'
model = "gpt-5.2"
model_reasoning_effort = "low"

[projects."/path/to/your/workspace"]
trust_level = "trusted"
EOF

# Optional override (the default is already ./.codex-gateway-home):
export CODEX_CLI_HOME=$PWD/.codex-gateway-home
```
