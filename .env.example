# Copy this file to `.env` (it is gitignored) and edit as needed.
#
# Then start the gateway with:
#   ./scripts/serve.sh

# Optional auth (if set, clients must send: Authorization: Bearer <token>)
CODEX_GATEWAY_TOKEN=devtoken

# Verbose gateway logs: prompt, codex events, response
CODEX_DEBUG_LOG=1
CODEX_LOG_MAX_CHARS=20000

# For clients that parse a single do(...)/finish(...) action (e.g. Open-AutoGLM)
CODEX_STRIP_ANSWER_TAGS=1

# Large prompts / screenshots can make codex JSONL events exceed default asyncio pipe limits
CODEX_SUBPROCESS_STREAM_LIMIT=16777216

# Prevent OpenAI SDK streaming read timeouts while Codex is thinking
CODEX_SSE_KEEPALIVE_SECONDS=2

# Prefer native vision over Codex's MCP-based image viewing tool (reduces MCP tool calls)
CODEX_DISABLE_VIEW_IMAGE_TOOL=1

# Optional: use Codex backend `/responses` API for all Codex requests (true token streaming).
# Vision requests auto-use `/responses` even if this is unset.
CODEX_USE_CODEX_RESPONSES_API=1
# CODEX_CODEX_BASE_URL=https://chatgpt.com/backend-api/codex
# CODEX_CODEX_VERSION=0.21.0
# CODEX_CODEX_USER_AGENT="codex_cli_rs/0.73.0 (Mac OS 26.0.1; arm64) Apple_Terminal/464"

# Default model and reasoning
CODEX_MODEL=gpt-5.2
CODEX_MODEL_REASONING_EFFORT=low
# Optional: force reasoning effort for all requests (overrides client-provided reasoning)
# CODEX_FORCE_REASONING_EFFORT=low

# Optional: accept alternative model ids from clients by mapping them to a real Codex model
# CODEX_MODEL_ALIASES={"autoglm-phone":"gpt-5.2"}
# Optional: customize GET /v1/models output
# CODEX_ADVERTISED_MODELS=gpt-5.2,gpt-5-codex

# Optional performance: point Codex at a minimal HOME to avoid starting MCP servers per request
# CODEX_CLI_HOME=/absolute/path/to/.codex-gateway-home

# Optional multi-provider (requires the corresponding CLIs installed & authenticated)
# Cursor Agent:
# CURSOR_AGENT_MODEL=sonnet-4-thinking
# CURSOR_AGENT_API_KEY=...   # or set CURSOR_API_KEY
#
# Claude Code:
# CLAUDE_MODEL=sonnet
#
# Gemini CLI:
# GEMINI_MODEL=auto
